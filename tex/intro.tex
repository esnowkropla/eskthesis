\chapter{Introduction}
Quantum computing is the study of computers which operate according to the laws of quantum mechanics rather than the laws of classical physics.  Quantum computers are theorized to be more efficient than classical computers because quantum algorithms for integer factorization\cite{shor} and global search\cite{grover} have been found that are asymptotically more efficient than the best known classical algorithms, although it is not proven that there are no equivalent classical algorithms.

Unlike classical computers, which operate on bits, quantum computers operate on quantum bits or qubits.  While classical information must take on a definite value (i.e. 0 or 1), quantum information can take on superpositions (the familiar quantum mechanics: $\frac{1}{\sqrt{2}}[\ket{0} + \ket{1}]$), as well as become entangled.  The laws of quantum mechanics, specifically the way in which Hilbert spaces behave for multiple particles, require that in order to fully describe a quantum system consisting of $n$ two-level particles we must record $2^n$ complex numbers.  Classical systems require only $O(n)$ numbers to be recorded to fully describe a system of $n$ particles.  This means that quantum systems require exponentially more computational resources than classical systems do, or alternatively that the computational power of the actual, quantum, universe is much larger than that of the apparent classical universe.

\section{Computational Complexity}
\label{sec:complex}
Computational Complexity theory is the branch of computer science dealing with the study of computational difficulty, and of classifying different computational problems according to their difficulty.  The problem of distinguishing the performance of a particular method of computation from the process of computation in the abstract is done via Turing machines.\cite{turing}  A Turing machine is a hypothetical computing device that is extremely simple to reason about; it consists only of a tape containing a grid of symbols, and a read head able to read and write symbols to the tape.  The machine's behaviour is controlled entirely by the symbol under the read head; based on the symbol the machine may write a new symbol in place, or move somewhere else on the tape.  The usefulness of Turing machines for understanding computation is encoded in the Church-Turing thesis:
\begin{center}
	\emph{A function is efficiently calculable only if it is computable by a Turing machine}
\end{center}
Informally, this assertion means that we only need concern ourselves with Turing machines, since other modes of computation are equivalent.

The computational resources a problem takes to solve is generally described using Big-O notation.\cite{qcbook}  For a given problem, we say that it is of order $O(g)$ if the number of steps a Turing machine must take to solve as a function of the input size is bounded from above by $g$.  For example, if a function $f$ takes $3T^2 - T$ steps for $T$ input then $f$ is $O(T^2)$ (dropping constant factors and trailing terms).  Equivalently we may say a Turing machine takes an amount of time proportional to $T^2$ to solve the problem by assuming each instruction carried out by the Turing machine takes a constant amount of time.

Computational problems are divided into \emph{complexity classes}.  Problems considered easy to solve are those which fall into class P (for Polynomial): those problems which are $O(n^d)$ for input sized $n$ and some constant $d$; that is, problems which require a polynomial number of steps.  Problems are considered hard to solve if they require an exponential number of steps, e.g. $O(2^n)$.

A special group of problems are those that fall into the \emph{NP} class: these are decision problems (that is, problems whose output is either yes or no) for which verifying whether or not a candidate solution is correct is in P.  Clearly, problems in P must also be in NP; if we can construct a solution in a polynomial amount of time then the time to verify is constant, which is in P.  It is one of the greatest open questions of the modern era whether P = NP, or if there exist problems for which verifying the solution is easy (i.e. in P) but solving is hard (not in P).

It was shown in 1971 by Cook\cite{sat} that there are problems in NP which any other problem in NP can be reduced to with only at most a polynomial overhead in difficulty.  As a result if such a problem were efficiently solvable then all NP problems would be (and thus P would equal NP).  These are known as NP-Complete problems.  Twenty-one such were famously demonstrated by Karp in 1972.\cite{karp}

Many problems of practical interest are known to be NP or NP-Complete.  As such, methods to solve them quickly are of much use.  The initial idea of using quantum mechanics to carry out computation is usually attributed to Feynman.\cite{feynman}  He noticed the difficulty in simulating quantum mechanics on classical computers due to the exponential resources required, and proposed using quantum mechanics to build a computer that could simulate physics.  This lead to the development of quantum computing.

\section{Quantum Computing}
Quantum computing is usually described using the so-called gate model.\cite{qc}  Some initial register of qubits is prepared, and then acted on sequentially by unitary operators known as quantum gates.  This sequence of quantum gates results in the input register of qubits being put into some sort of quantum state, generally a mixture of superposition and entanglement.  Finally the qubits are measured, resulting in a classical vector of information as output depending on which wave-function component is picked out by the measurement.  The number of gates required to go from the specified input to the desired output maps analogously to the number of classical gates required for a classical digital computation, and in the same way that counting classical gates allows us to form big-O asymptotic bounds on computation time we can asymptotically bound the running time of a quantum algorithm by counting the number of unitary gates it requires.

Interest in quantum computers expanded with the discovery of an algorithm that was provably asymptotically faster than any classical algorithm: the Deutsch-Josza algorithm.\cite{deutsch}  The Deutsch-Jozsa algorithm solves a highly contrived problem: given $n$ input bits that are guaranteed to be one of

\begin{center}
	\begin{tabular}{l l}
		1) & equally partitioned between $n/2$ zeros and $n/2$ ones (balanced) \\
		2) & contain all zeros (constant) \\
		3) & contain all ones (constant) \\
	\end{tabular}
\end{center}
determine whether the input is balanced or constant.  The fastest classical algorithm is to simply inspect half of the elements plus one, $n/2 + 1$ operations; meanwhile, the Deutsch-Josza algorithm can determine whether the input is balanced or constant in only a constant number of operations.  This problem is not very interesting on its own, but what \emph{is} of interest is that a computer that can take advantage of quantum operations can solve it faster than a purely classical computer.

The discovery of a quantum algorithm asymptotically faster than the best classical one led to a surge of research, resulting in the discoveries of the Quantum Fourier Transform (QFT)\cite{qcbook}, Shor's Algorithm\cite{shor} and Grover's Algorithm\cite{grover}.

Building a quantum computer is a difficult task;\cite{qc}\cite{qcbook} in order for the computer to remain well described by quantum mechanics, interaction with the outside world must be minimized otherwise the computational elements will become thermally mixed and no longer behave quantum mechanically.  However, a computer must interact with the outside world to receive input and return output.  This means that the computational section of a quantum computer cannot be too isolated.  There exist quantum circuit elements that do \emph{quantum error-correction},\cite{shor_error} which can counteract the deleterious effect of thermal noise or other sources of decoherence.  Unfortunately the larger a quantum circuit grows the more vulnerable to decoherence it becomes, and quantum error correcting schemes add qubits to the circuits which they error correct.  These extra qubits bring in more decoherence; at present, more decoherence than the error-correcting codes fix.\cite{qcbook}

\section{Outline}
One proposed method for solving this runaway decoherence problem is \emph{adiabatic quantum computing} (AQC).\cite{farhi}  AQC is essentially a form of analog computing, where a physical system is set up in such a way that as it evolves according to the laws of physics it naturally computes the solution to a problem.  This dissertation will cover the background of AQC as well as explore the process of computing on an actual adiabatic quantum computer, the ``D-Wave Two'', including the requirements of compiling problems into a specific machine solvable form.  We will also look at the experimental results of solving some actual problems on the D-Wave Two.
